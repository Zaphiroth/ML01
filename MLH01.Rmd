---
title: "Machine Learning Homework 01"
author: "Liu Zhe"
date: "2022-10-08"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = TRUE,
  warning = FALSE
)
```

## Environment Variables and Packages

```{r}
options(java.parameters = "-Xmx2048m",
        stringsAsFactors = FALSE, 
        encoding = 'UTF-8')

suppressPackageStartupMessages({
  # DM
  library(zip)
  library(openxlsx)
  library(readxl)
  library(writexl)
  library(RcppRoll)
  library(plyr)
  library(stringi)
  library(feather)
  library(RODBC)
  library(MASS)
  library(car)
  library(data.table)
  library(tidyverse)
  library(lubridate)
  # TS
  library(tseries)
  library(forecast)
  library(lmtest)
  library(FinTS)
  # DT
  library(party)
  library(partykit)
  library(rpart)
  # RF
  library(randomForest)
  # plot
  library(isotone)
})
```

## E-commerce Fresh

建立ARIMA模型需要时间序列数据差分平稳，同时不能为白噪声数据，否则没有分析价值。

首先读取数据，并查看每列的数据类型。

```{r}
fresh.raw <- read_csv('Tseq_Sales.csv')

head(fresh.raw)
```

因为date列的字符串长度不一致，无法对数据按date列排序，会影响时间序列数据的时间顺序，所以先将其统一为yyyy/mm/dd格式，再进行排序。

```{r}
fresh.clean <- fresh.raw %>% 
  separate(date, into = c('year', 'month', 'day'), sep = '/') %>% 
  mutate(month = stri_pad_left(month, 2, 0), 
         day = stri_pad_left(day, 2, 0)) %>% 
  unite('date', year, month, day, sep = '/') %>% 
  arrange(is_train, date)

head(fresh.clean)
```

根据is_train划分训练集和测试集。

```{r}
fresh.train <- filter(fresh.clean, is_train == 0)
fresh.test <- filter(fresh.clean, is_train == 1)
```

训练集数据的时间序列图如下，可以看出序列并不平稳。考虑拟合ARIMA模型、残差自回归模型和广义线性模型。

```{r}
plot(ts(fresh.train$sales), ylab = 'Sales')
```

# ARIMA模型

对sales数据进行一阶差分，根据时间序列图和ACF图，判断数据一阶差分平稳。同时ADF检验的结果也认为一阶差分数据平稳。

```{r}
fresh.sta <- fresh.train %>% 
  mutate(sales_sta = sales - lag(sales)) %>% 
  filter(!is.na(sales_sta)) %>% 
  select(date, sales_sta)

plot(ts(fresh.sta$sales_sta), ylab = 'Sales-1st')
fresh.acf <- acf(fresh.sta$sales_sta, lag.max = 30)
adf.test(fresh.sta$sales_sta, k = 1)
```

使用LB统计量对一阶差分平稳数据做白噪声检验，认为数据非白噪声，具有研究价值。

```{r}
Box.test(fresh.sta$sales_sta, lag = 1, type = 'Ljung-Box')
```

由ACF图和PACF图，认为数据ACF一阶截尾，PACF拖尾。

```{r}
fresh.arima.acf <- acf(fresh.sta$sales_sta, lag.max = 30)
fresh.arima.pacf <- pacf(fresh.sta$sales_sta, lag.max = 30)
```

因此，拟合ARIMA(0,1,1)模型。

```{r}
fresh.arima.fit <- arima(fresh.train$sales, order = c(0, 1, 1))
summary(fresh.arima.fit)
```

对ARIMA(0,1,1)模型的残差序列进行白噪声检验，认为残差序列为白噪声序列，拟合模型显著。

```{r}
Box.test(fresh.arima.fit$residuals, lag = 1, type = 'Ljung-Box')
```

对ARIMA(0,1,1)模型的参数进行显著性检验，认为参数显著非零。

```{r}
CoefTestFunc <- function(object) {
  coef <- object$coef[object$coef != 0]
  var.coef <- object$var.coef
  len <- length(coef)
  df <- object$nobs - len
  for (i in 1:len) {
    t <- coef[i] / sqrt(var.coef[i, i])
    lower <- ifelse(coef[i] < 0, 1, 0)
    pt <- pt(t, df = df, lower.tail = lower)
    print(pt)
  }
}

CoefTestFunc(fresh.arima.fit)
```

使用ARIMA(0,1,1)模型对测试集数据进行预测时，固定第一期的时间序列值，根据模型表达式依次预测将来期的序列值，并将负数的预测值修改为0。

```{r}
fresh.arima.pred <- mutate(fresh.test, sales_pred = sales[1])
for (i in 2:nrow(fresh.arima.pred)) {
  pred <- fresh.arima.pred$sales_pred[i-1]
  fresh.arima.pred$sales_pred[i] <- ifelse(pred > 0, pred, 0)
}

head(fresh.arima.pred)
```

预测效果如下：

```{r}
plot(ts(fresh.arima.pred$sales), ylab = 'Sales')
lines(ts(fresh.arima.pred$sales_pred), col = 2)
```

## 残差自回归模型

从训练数据的时间序列图可以看出，时间序列有明显的先升后降趋势，可以先通过确定性因素分解方法提取序列中主要的确定性信息，再进一步拟合残差自回归模型提取相关信息。

因为数据的时间跨度不够长，无法提取季节效应，所以考虑用时间t的幂函数提取趋势效应。

```{r}
t <- 1:nrow(fresh.train)
t2 <- t^2
fresh.lm.fit1 <- lm(fresh.train$sales ~ t + t2)
summary(fresh.lm.fit1)
```

然后，拟合关于延迟变量的自回归模型。

```{r}
fresh.sales1 <- fresh.train$sales[1:(nrow(fresh.train) - 1)]
fresh.sales2 <- fresh.train$sales[2:nrow(fresh.train)]
fresh.lm.fit2 <- lm(fresh.sales1 ~ fresh.sales2)
summary(fresh.lm.fit2)
```

两个趋势拟合模型的拟合效果图。

```{r}
fresh.lm.fv1 <- ts(fresh.lm.fit1$fitted.values)
fresh.lm.fv2 <- ts(fresh.lm.fit2$fitted.values, start = 2)
plot(ts(fresh.train$sales))
lines(fresh.lm.fv1, col = 2)
lines(fresh.lm.fv2, col = 4)
```

对第一个确定性趋势进行DW检验，由于DW统计量小于2，且P值极小，说明残差序列高度正相关，需要对残差序列再次进行信息提取。

```{r}
dwtest(fresh.lm.fit1)
```

对延迟因变量自相关模型进行Durbin h检验，认为残差序列不存在显著相关，不需要对残差序列进行信息提取。

```{r}
dwtest(fresh.lm.fit2, order.by = fresh.sales2)
```

由第一个确定性趋势残差序列的ACF图和PACF图，认为残差序列ACF拖尾，PACF一阶、五阶相关性较高。

```{r}
fresh.rar.acf <- acf(fresh.lm.fit1$residuals, lag.max = 30)
fresh.rar.pacf <- pacf(fresh.lm.fit1$residuals, lag.max = 30)
```

因此，拟合ARIMA((1,5),0,0)模型。

```{r}
fresh.rar.fit <- arima(fresh.lm.fit1$residuals, 
                       order = c(5, 0, 0), 
                       include.mean = FALSE, 
                       transform.pars = FALSE, 
                       fixed = c(NA, 0, 0, 0, NA))
summary(fresh.rar.fit)
```

对ARIMA((1,5),0,0)模型的残差序列进行白噪声检验，认为残差序列为白噪声序列，拟合模型显著。

```{r}
Box.test(fresh.rar.fit$residuals, lag = 1, type = 'Ljung-Box')
```

对ARIMA((1,5),0,0)模型的参数进行显著性检验，认为参数显著非零。

```{r}
CoefTestFunc(fresh.rar.fit)
```

使用残差自回归模型对测试集数据进行预测时，固定前五期的时间序列值，根据模型表达式依次预测将来期的序列值，并将负数的预测值修改为0。

```{r}
fresh.rar.coef1 <- fresh.lm.fit1$coefficients[1]
fresh.rar.coef2 <- fresh.lm.fit1$coefficients[2]
fresh.rar.coef3 <- fresh.lm.fit1$coefficients[3]

fresh.rar.pred <- fresh.test %>% 
  mutate(t = row_number()) %>% 
  mutate(sales_pred = fresh.rar.coef1 + fresh.rar.coef2 * t + fresh.rar.coef3 * t^2, 
         sales_pred_ad = ifelse(sales_pred > 0, sales_pred, 0))

head(fresh.rar.pred)
```

预测效果如下：

```{r}
plot(ts(fresh.rar.pred$sales), ylab = 'Sales')
lines(ts(fresh.rar.pred$sales_pred_ad), col = 2)
```

## 广义线性模型

假设数据满足Gauss假定，则可以拟合关于时间t的线性回归模型。

```{r}
t <- 1:nrow(fresh.train)
t2 <- t^2
t3 <- t^3
t4 <- t^4
t5 <- t^5
fresh.glm.fit <- lm(fresh.train$sales ~ t + t2 + t3 + t4 + t5, family = gaussian())
summary(fresh.glm.fit)
```

可以看出拟合模型显著，且参数显著非零。拟合效果如下：

```{r}
plot(fresh.train$sales, type = 'p', pch = 20)
lines(fresh.glm.fit$fitted.values, col = 2)
```

使用线性回归模型对测试集数据进行预测。

```{r}
GLMPredFunc <- function(t, object) {
  coef <- object$coefficients
  pred <- sum(c(1, t, t^2, t^3, t^4, t^5) * coef)
  return(pred)
}

fresh.glm.pred <- fresh.test %>% 
  mutate(t = row_number(), 
         sales_pred = sapply(t, GLMPredFunc, object = fresh.glm.fit))

head(fresh.glm.pred)
```

预测效果如下：

```{r}
plot(ts(fresh.glm.pred$sales), ylab = 'Sales')
lines(ts(fresh.glm.pred$sales_pred), col = 2)
```

# MSE和柔性的关系

计算三种模型训练结果的MSE和柔性。

```{r}
fresh.arima.train <- mutate(fresh.train, sales_pred = sales[1])
for (i in 2:nrow(fresh.arima.train)) {
  pred <- fresh.arima.train$sales_pred[i-1]
  fresh.arima.train$sales_pred[i] <- ifelse(pred > 0, pred, 0)
}

fresh.rar.train <- fresh.train %>% 
  mutate(t = row_number()) %>% 
  mutate(sales_pred = fresh.rar.coef1 + fresh.rar.coef2 * t + fresh.rar.coef3 * t^2, 
         sales_pred_ad = ifelse(sales_pred > 0, sales_pred, 0))

fresh.glm.train <- fresh.train %>% 
  mutate(t = row_number(), 
         sales_pred = sapply(t, GLMPredFunc, object = fresh.glm.fit))

fresh.arima.train.mse <- mean((fresh.arima.train$sales - fresh.arima.train$sales_pred)^2)
fresh.rar.train.mse <- mean((fresh.rar.train$sales - fresh.rar.train$sales_pred_ad)^2)
fresh.glm.train.mse <- mean((fresh.glm.train$sales - fresh.glm.train$sales_pred)^2)
```

计算三种模型预测结果的MSE和柔性。

```{r}
fresh.arima.test.mse <- mean((fresh.arima.pred$sales - fresh.arima.pred$sales_pred)^2)
fresh.rar.test.mse <- mean((fresh.rar.pred$sales - fresh.rar.pred$sales_pred_ad)^2)
fresh.glm.test.mse <- mean((fresh.glm.pred$sales - fresh.glm.pred$sales_pred)^2)
```

训练数据和测试数据的MSE随模型柔性变化的图为：

```{r}
mse.flex <- data.frame(flexibility = c(length(fresh.arima.fit$coef), 
                                       length(fresh.rar.fit$coef), 
                                       length(fresh.glm.fit$coefficients)), 
                       train = c(fresh.arima.train.mse, 
                                 fresh.rar.train.mse, 
                                 fresh.glm.train.mse), 
                       test = c(fresh.arima.test.mse, 
                                fresh.rar.test.mse, 
                                fresh.glm.test.mse))
mse.flex

plot(0: 8, seq(0, 8000, 1000), type = 'n', xlab = 'Flexibility', ylab = 'MSE')
points(x = mse.flex$flexibility, y = mse.flex$train, type = 'p', pch = 4)
lines(x = mse.flex$flexibility, y = mse.flex$train, col = 2)
points(x = mse.flex$flexibility, y = mse.flex$test, type = 'p', pch = 4)
lines(x = mse.flex$flexibility, y = mse.flex$test, col = 4)
legend('right', c('train', 'test'), col = c(2, 4), lty=c(1, 1), bty="n")
```

由于测试数据与训练数据差别过于巨大，导致残差自回归模型和广义线性回归模型在测试集数据上的表现极差；虽然ARIMA模型柔性最低，训练误差最大，但在测试数据上有最好的表现。

## Ink

# 读取数据

将数据集分为训练数据（瓶装）、训练数据（简装）和测试数据三部分，添加类别标签。

```{r}
ink.raw <- read_table('inks5_CLASSdataset.txt', show_col_types = FALSE)

head(ink.raw)
```

```{r}
ink.train1 <- ink.raw %>% 
  filter(Itemtype == 'TRAIN', stri_sub(Name, -2, -1) != '.1') %>% 
  mutate(label = as.factor(stri_sub(Name, -1, -1)))

head(ink.train1)
```

```{r}
ink.train2 <- ink.raw %>% 
  filter(Itemtype == 'TRAIN', stri_sub(Name, -2, -1) == '.1') %>% 
  mutate(label = as.factor(stri_sub(Name, -3, -3)))

head(ink.train2)
```

```{r}
ink.test <- ink.raw %>% 
  filter(Itemtype == 'TEST') %>% 
  mutate(label = as.factor(stri_sub(Name, -1, -1)))

head(ink.test)
```

# 模型选择

经过对逻辑回归、决策树、随机森林等分类模型的计算和比较，选择训练效果和预测效果最好的随机森林分类模型。

随机森林分类模型中有两个主要的超参数：构建决策树分支时抽取的变量个数（mtry）和随机森林中树的数量（ntree）。超参数的设置会在很大程度上影响模型的预测效果。

选择mtry范围为1~3（数据中共有3个变量），ntree范围为100~1000（间隔100），两两组合成30组超参数对，每组训练10个随机森林模型，比较各个模型的错误率。

# 确定训练数据（瓶装）的超参数

首先利用训练数据（瓶装）训练随机森林分类模型，结果显示mtry=1，ntree=100时，模型已经有比较低的训练错误率和测试错误率。

```{r}
ink.rf1.df <- data.frame(num = rep(1:10, each = 30), 
                         mtry = rep(1:3, each = 10, times = 10), 
                         ntree = rep((1:10) * 100, times = 30), 
                         obb = NA, 
                         test = NA)

set.seed(1)
for (i in 1:10) {
  for (j in 1:3) {
    for (k in seq(100, 1000, 100)) {
      rf.fit <- randomForest(
        label ~ x + y + z, data = ink.train1, 
        mtry = j, ntree = k, 
        xtest = ink.test[4:6], ytest = ink.test$label, 
        importance = TRUE
      )
      ink.rf1.df$obb[ink.rf1.df$num == i & ink.rf1.df$mtry == j & ink.rf1.df$ntree == k] <- mean(rf.fit$confusion[, 6])
      ink.rf1.df$test[ink.rf1.df$num == i & ink.rf1.df$mtry == j & ink.rf1.df$ntree == k] <- mean(rf.fit$test$confusion[, 6])
    }
  }
}

ink.rf1.df %>% arrange(test) %>% head(10)
```

设置超参数mtry=1，ntree=100训练随机森林分类模型。

```{r}
set.seed(10)
ink.rf1.fit <- randomForest(label ~ x + y + z, data = ink.train1, 
                            mtry = 1, ntree = 100, 
                            xtest = ink.test[4:6], ytest = ink.test$label, 
                            importance = TRUE)
ink.rf1.fit
```

# 确定训练数据（简装）的超参数

下面利用训练数据（简装）训练随机森林分类模型，结果显示mtry=2，ntree=100时，模型已经有比较低的训练错误率和测试错误率。

```{r}
ink.rf2.df <- data.frame(num = rep(1:10, each = 30), 
                         mtry = rep(1:3, each = 10, times = 10), 
                         ntree = rep((1:10) * 100, times = 30), 
                         obb = NA, 
                         test = NA)

set.seed(20)
for (i in 1:10) {
  for (j in 1:3) {
    for (k in seq(100, 1000, 100)) {
      rf.fit <- randomForest(
        label ~ x + y + z, data = ink.train2, 
        mtry = j, ntree = k, 
        xtest = ink.test[4:6], ytest = ink.test$label, 
        importance = TRUE
      )
      ink.rf2.df$obb[ink.rf2.df$num == i & ink.rf2.df$mtry == j & ink.rf2.df$ntree == k] <- mean(rf.fit$confusion[, 6])
      ink.rf2.df$test[ink.rf2.df$num == i & ink.rf2.df$mtry == j & ink.rf2.df$ntree == k] <- mean(rf.fit$test$confusion[, 6])
    }
  }
}

ink.rf2.df %>% arrange(test) %>% head(10)
```

设置超参数mtry=1，ntree=100训练随机森林分类模型。

```{r}
set.seed(50)
ink.rf2.fit <- randomForest(label ~ x + y + z, data = ink.train1, 
                            mtry = 2, ntree = 100, 
                            xtest = ink.test[4:6], ytest = ink.test$label, 
                            importance = TRUE)
ink.rf2.fit
```

# 单一墨迹误差

利用训练数据（瓶装）训练的随机森林分类模型在测试数据上的正确率优于利用训练数据（简装）训练的随机森林分类模型。下面分析瓶装模型的误差。

模型每一种墨迹的训练误差为

```{r}
ink.train1.pred <- ink.train1 %>% 
  mutate(label_pred = ink.rf1.fit$predicted) %>% 
  group_by(Name) %>% 
  summarise(error = mean(as.numeric(label) - as.numeric(label_pred))^2) %>% 
  ungroup()

ink.train1.pred
```

模型每一种墨迹的测试误差为

```{r}
ink.test1.pred <- ink.test %>% 
  mutate(label_pred = ink.rf1.fit$test$predicted) %>% 
  group_by(Name) %>% 
  summarise(error = mean(as.numeric(label) - as.numeric(label_pred))^2) %>% 
  ungroup()

ink.test1.pred
```

模型每一种墨迹的微训练误差为

```{r}
ink.train1.label <- ink.train1 %>% 
  mutate(prop = 1) %>% 
  pivot_wider(names_from = label, 
              values_from = prop, 
              values_fill = 0) %>% 
  select(`1`, `2`, `3`, `4`, `5`) %>% 
  as.matrix()

ink.train1.pred.m <- data.frame(Name = sort(unique(ink.train1$Name)), 
                                error = colMeans((ink.rf1.fit$votes - ink.train1.label)^2))

ink.train1.pred.m
```

模型每一种墨迹的微测试误差为

```{r}
ink.test1.label <- ink.test %>% 
  mutate(prop = 1) %>% 
  pivot_wider(names_from = label, 
              values_from = prop, 
              values_fill = 0) %>% 
  select(`1`, `2`, `3`, `4`, `5`) %>% 
  as.matrix()

ink.test1.pred.m <- data.frame(Name = sort(unique(ink.test$Name)), 
                                error = colMeans((ink.rf1.fit$test$votes - ink.test1.label)^2))

ink.test1.pred.m
```

模型整体的微训练误差为：

```{r}
mean((ink.rf1.fit$votes - ink.train1.label)^2)
```

模型整体的微测试误差为：

```{r}
mean((ink.rf1.fit$votes - ink.test1.label)^2)
```

# 四种图

Histogram

```{r}
source('Histogram.R')

h1 <- ink.rf1.fit$test$votes[1:10, 1]
h2 <- 1 - ink.rf1.fit$test$votes[1:10, 1]
histograms(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[21:30, 2]
h2 <- 1 - ink.rf1.fit$test$votes[21:30, 2]
histograms(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[41:50, 3]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
histograms(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[31:40, 4]
h2 <- 1 - ink.rf1.fit$test$votes[31:40, 4]
histograms(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[11:20, 5]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
histograms(LR.H1.exp = h1, LR.H2.exp = h2)
```

Tippett

```{r}
source('Tippett.R')

h1 <- ink.rf1.fit$test$votes[1:10, 1]
h2 <- 1 - ink.rf1.fit$test$votes[1:10, 1]
Tippett(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[21:30, 2]
h2 <- 1 - ink.rf1.fit$test$votes[21:30, 2]
Tippett(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[41:50, 3]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
Tippett(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[31:40, 4]
h2 <- 1 - ink.rf1.fit$test$votes[31:40, 4]
Tippett(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[11:20, 5]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
Tippett(LR.H1.exp = h1, LR.H2.exp = h2)
```

DET

```{r}
source('DET.R')

h1 <- ink.rf1.fit$test$votes[1:10, 1]
h2 <- 1 - ink.rf1.fit$test$votes[1:10, 1]
DET(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[21:30, 2]
h2 <- 1 - ink.rf1.fit$test$votes[21:30, 2]
DET(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[41:50, 3]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
DET(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[31:40, 4]
h2 <- 1 - ink.rf1.fit$test$votes[31:40, 4]
DET(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[11:20, 5]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
DET(LR.H1.exp = h1, LR.H2.exp = h2)
```

ECE

```{r}
source('ECE.R')

h1 <- ink.rf1.fit$test$votes[1:10, 1]
h2 <- 1 - ink.rf1.fit$test$votes[1:10, 1]
ECE(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[21:30, 2]
h2 <- 1 - ink.rf1.fit$test$votes[21:30, 2]
ECE(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[41:50, 3]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
ECE(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[31:40, 4]
h2 <- 1 - ink.rf1.fit$test$votes[31:40, 4]
ECE(LR.H1.exp = h1, LR.H2.exp = h2)

h1 <- ink.rf1.fit$test$votes[11:20, 5]
h2 <- 1 - ink.rf1.fit$test$votes[41:50, 3]
ECE(LR.H1.exp = h1, LR.H2.exp = h2)
```
